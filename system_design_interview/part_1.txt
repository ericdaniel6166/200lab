Database to use: SQL DB vs NoSQL DB
    SQL DB
        AWS
            RDS
            Aurora
    NoSQL DB
        4 categories
            key-value stores
            graph stores
            column stores
            document stores
        use case
            • Your application requires super-low latency.
            • Your data are unstructured, Or you do not have any relational data,
            • You only need to serialize and deserialize data (JSON. XML, YAML, etc.).
            • You need to store a massive amount of data.
------
vertical scaling vs horizontal scaling
    • Vertical scaling has a hard limit. It is impossible to add unlimited CPU and memory to a server.
    • Vertical scaling does not have failover and redundancy.
    If one Server goes down, the website/app goes down with it completely.
------
Load balancer
A load balancer evenly distributes incoming traffic among web Servers

• If server 1 goes offline, all the traffic will be routed to server 2. This prevents the website
from going offline. We will also add a new healthy web server to the server pool to balance the load.
• If the website traffic grows rapidly. and two servers are not enough to handle the traffic,
the load balancer can handle this problem gracefully. You only need to add more servers
to the web server pool, and the load balancer automatically starts to send requests to them..
------
Database replication
Database replication is a master/slave relationship between the original (master) and the copies
(slaves)

A master database generally only supports write operations. A slave database gets copies of
the data from the master database and only Supports read operations. All the data-modifying
commands like insert, delete, or update must be sent to the master database. Most
applications require a much higher ratio of reads to writes; thus, the number of slave
databases in a system is usually larger than the number of master databases.

Advantages of database replication:
• Better performance: In the master-slave model, all writes and updates happen in master
nodes; whereas. read operations are distributed across slave nodes. This model improves
performance because it allows more queries to be processed in parallel.
• Reliability: If one of your database servers is destroyed by a natural disaster, such as a
typhoon or an earthquake. data is still preserved. You do need to worry about data loss
because data is replicated across multiple locations.
• High availability: By replicating data across different locations, your website remains in
operation even if a database is offline as you can access data stored in another database

If one of the database goes offline
• If only one slave database is available, and it goes offline. read operations will be directed
to the master database temporarily. As soon as the issue is found, a new slave database
will replace the old one. In case multiple slave databases are available, read operations are
redirected to other healthy slave databases. A new database server will replace the old one.
• If the master database goes offline, a slave database will be promoted to be the new
master. All the database operations will be temporarily executed on the new master
database. A new slave database will replace the old one data replication immediately.
In production systems, promoting a new master is more complicated as the data in a slave
database might not be up to date. The missing data needs to updated by running data
recovery scripts. Although some other replication methods like multi-masters and circular
replication could help.
------
Cache
A cache is a temporary Storage area that Stores the result Of expensive frequently
accessed data in memory so that subsequent requests are served more quickly.
---
Cache tier
The cache tier is a temporary data store layer, much faster than the database. The benefits of
having a separate cache tier include better system performance, ability to reduce database
workloads, and the ability to scale the cache tier independently.

After receiving a request, a web server first checks if the cache has the available response. If
it has, it sends data back to the client. If not, it queries the database, stores the response in
cache, and sends it back to the client. This caching strategy is called a read-through cache.
Other caching strategies are available depending on the data type, size, and access patterns.
---
Considerations for using cache
Here are a few considerations for using a cache system:
• Decide when to use cache. Consider using cache when data is read frequently but
modified infrequently. Since cached data is stored in volatile memory, a cache server is
not ideal for persisting data, For instance, if a cache server restarts, all the data in memory
is lost. Thus, important data should be saved in persistent data stores.
• Expiration policy. It is a good practice to implement an expiration policy. Once cached
data is expired, it is removed from the cache. When there is no expiration policy, cached
data will be stored in the memory permanently. It is advisable not to make the expiration
date too short as this will cause the system to reload data from the database too frequently.
Meanwhile, it is advisable not to make the expiration date too long as the data can become stale.
• Consistency: This involves keeping the data store and the cache in sync. Inconsistency
can happen because data-modifying operations on the data store and cache are not in a
single transaction. When scaling across multiple regions, maintaining consistency between
the data store and cache is challenging.
• Mitigating failures: A single cache server represents a potential single point of failure
(SPOF), defined in Wikipedia as follows: "A single point of failure (SPOF) is a part of a
system that, if it fails, will stop the entire system from working" [8]. As a result, multiple
cache servers across different data centers are recommended to avoid SPOF. Another
recommended approach is to overprovision the required memory by certain percentages.
This provides a buffer as the memory usage increases.
• Eviction Policy: Once the cache is full, any requests to add items to the cache might
cause existing items to be removed. This is called cache eviction. Least-recently-used
(LRU) is the most popular cache eviction policy. Other eviction policies, such as the Least
Frequently Used (LFU) or First in First Out (FIFO), can be adopted to satisfy different use cases.
------
Content delivery network (CDN)
A CDN is a network of geographically dispersed servers used to deliver static content. CDN
servers cache static content like images, videos, CSS, JavaScript files, etc.
---
Considerations of using a CDN
• Cost: CDNs are run by third-party providers, and you are charged for data transfers in
and out of the CDN. Caching infrequently used assets provides no significant benefits so
you should consider moving them out of the CDN.
• Setting an appropriate cache expiry: For time-sensitive content, setting a cache expiry
time is important. The cache expiry time should neither be too long nor too short. If it is
too long, the content might no longer be fresh. If it is too short, it can cause repeat
reloading of content from origin servers to the CDN.
• CDN fallback: You should consider how your website/application copes with CDN
failure. If there is a temporary CDN outage, clients should be able to detect the problem
and request resources from the origin.
• Invalidating files: You can remove a file from the CDN before it expires by performing
one of the following operations:
    • Invalidate the CDN object using APIs provided by CDN vendors.
    • Use object versioning to serve a different version of the object. To version an object,
    you can add a parameter to the URL, such as a version number. For example, version
    number 2 is added to the query string: image.png?v=2.
---
---
------
------
------
------

